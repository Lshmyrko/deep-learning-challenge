{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt6dmbg/d+qr2unQ1NH85d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lshmyrko/deep-learning-challenge/blob/main/AlphabetSoupCharity_Optimization.h5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the data\n",
        "url = \"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\"\n",
        "application_df = pd.read_csv(url)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "application_df = application_df.drop(columns=['EIN','NAME'])\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = application_df.drop(\"IS_SUCCESSFUL\", axis=1)\n",
        "y = application_df[\"IS_SUCCESSFUL\"]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
        "\n",
        "# Define preprocessing for numerical and categorical data\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Build the model with adjusted architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(units=256, activation='relu', input_dim=preprocessor.fit_transform(X_train).shape[1]))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=16, activation='relu'))  # Additional hidden layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Use Adam optimizer with a lower learning rate\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "model.fit(X_train_transformed, y_train, epochs=40, batch_size=64, verbose=2)\n",
        "\n",
        "# Evaluate the model\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "model_loss, model_accuracy = model.evaluate(X_test_transformed, y_test, verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avZBT7gXnitU",
        "outputId": "2db305f5-2196-4bba-b077-ab802011c616"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "402/402 - 3s - loss: 0.6568 - accuracy: 0.6219 - 3s/epoch - 8ms/step\n",
            "Epoch 2/40\n",
            "402/402 - 2s - loss: 0.5973 - accuracy: 0.7107 - 2s/epoch - 4ms/step\n",
            "Epoch 3/40\n",
            "402/402 - 2s - loss: 0.5798 - accuracy: 0.7223 - 2s/epoch - 4ms/step\n",
            "Epoch 4/40\n",
            "402/402 - 2s - loss: 0.5738 - accuracy: 0.7227 - 2s/epoch - 4ms/step\n",
            "Epoch 5/40\n",
            "402/402 - 2s - loss: 0.5706 - accuracy: 0.7262 - 2s/epoch - 5ms/step\n",
            "Epoch 6/40\n",
            "402/402 - 2s - loss: 0.5675 - accuracy: 0.7262 - 2s/epoch - 6ms/step\n",
            "Epoch 7/40\n",
            "402/402 - 2s - loss: 0.5623 - accuracy: 0.7287 - 2s/epoch - 4ms/step\n",
            "Epoch 8/40\n",
            "402/402 - 2s - loss: 0.5614 - accuracy: 0.7292 - 2s/epoch - 4ms/step\n",
            "Epoch 9/40\n",
            "402/402 - 2s - loss: 0.5605 - accuracy: 0.7287 - 2s/epoch - 4ms/step\n",
            "Epoch 10/40\n",
            "402/402 - 2s - loss: 0.5577 - accuracy: 0.7293 - 2s/epoch - 4ms/step\n",
            "Epoch 11/40\n",
            "402/402 - 2s - loss: 0.5574 - accuracy: 0.7298 - 2s/epoch - 4ms/step\n",
            "Epoch 12/40\n",
            "402/402 - 3s - loss: 0.5564 - accuracy: 0.7304 - 3s/epoch - 6ms/step\n",
            "Epoch 13/40\n",
            "402/402 - 2s - loss: 0.5553 - accuracy: 0.7304 - 2s/epoch - 5ms/step\n",
            "Epoch 14/40\n",
            "402/402 - 2s - loss: 0.5538 - accuracy: 0.7330 - 2s/epoch - 4ms/step\n",
            "Epoch 15/40\n",
            "402/402 - 2s - loss: 0.5529 - accuracy: 0.7334 - 2s/epoch - 4ms/step\n",
            "Epoch 16/40\n",
            "402/402 - 2s - loss: 0.5529 - accuracy: 0.7315 - 2s/epoch - 4ms/step\n",
            "Epoch 17/40\n",
            "402/402 - 2s - loss: 0.5522 - accuracy: 0.7337 - 2s/epoch - 4ms/step\n",
            "Epoch 18/40\n",
            "402/402 - 2s - loss: 0.5506 - accuracy: 0.7330 - 2s/epoch - 5ms/step\n",
            "Epoch 19/40\n",
            "402/402 - 3s - loss: 0.5509 - accuracy: 0.7334 - 3s/epoch - 6ms/step\n",
            "Epoch 20/40\n",
            "402/402 - 2s - loss: 0.5497 - accuracy: 0.7344 - 2s/epoch - 4ms/step\n",
            "Epoch 21/40\n",
            "402/402 - 2s - loss: 0.5488 - accuracy: 0.7348 - 2s/epoch - 4ms/step\n",
            "Epoch 22/40\n",
            "402/402 - 2s - loss: 0.5496 - accuracy: 0.7361 - 2s/epoch - 6ms/step\n",
            "Epoch 23/40\n",
            "402/402 - 2s - loss: 0.5482 - accuracy: 0.7334 - 2s/epoch - 5ms/step\n",
            "Epoch 24/40\n",
            "402/402 - 3s - loss: 0.5468 - accuracy: 0.7373 - 3s/epoch - 7ms/step\n",
            "Epoch 25/40\n",
            "402/402 - 2s - loss: 0.5471 - accuracy: 0.7347 - 2s/epoch - 6ms/step\n",
            "Epoch 26/40\n",
            "402/402 - 2s - loss: 0.5477 - accuracy: 0.7345 - 2s/epoch - 4ms/step\n",
            "Epoch 27/40\n",
            "402/402 - 2s - loss: 0.5465 - accuracy: 0.7352 - 2s/epoch - 4ms/step\n",
            "Epoch 28/40\n",
            "402/402 - 2s - loss: 0.5461 - accuracy: 0.7373 - 2s/epoch - 4ms/step\n",
            "Epoch 29/40\n",
            "402/402 - 2s - loss: 0.5457 - accuracy: 0.7358 - 2s/epoch - 4ms/step\n",
            "Epoch 30/40\n",
            "402/402 - 2s - loss: 0.5448 - accuracy: 0.7361 - 2s/epoch - 4ms/step\n",
            "Epoch 31/40\n",
            "402/402 - 2s - loss: 0.5456 - accuracy: 0.7371 - 2s/epoch - 5ms/step\n",
            "Epoch 32/40\n",
            "402/402 - 2s - loss: 0.5453 - accuracy: 0.7358 - 2s/epoch - 5ms/step\n",
            "Epoch 33/40\n",
            "402/402 - 2s - loss: 0.5442 - accuracy: 0.7360 - 2s/epoch - 4ms/step\n",
            "Epoch 34/40\n",
            "402/402 - 2s - loss: 0.5452 - accuracy: 0.7360 - 2s/epoch - 4ms/step\n",
            "Epoch 35/40\n",
            "402/402 - 2s - loss: 0.5438 - accuracy: 0.7369 - 2s/epoch - 4ms/step\n",
            "Epoch 36/40\n",
            "402/402 - 2s - loss: 0.5442 - accuracy: 0.7370 - 2s/epoch - 4ms/step\n",
            "Epoch 37/40\n",
            "402/402 - 2s - loss: 0.5440 - accuracy: 0.7372 - 2s/epoch - 4ms/step\n",
            "Epoch 38/40\n",
            "402/402 - 3s - loss: 0.5438 - accuracy: 0.7363 - 3s/epoch - 6ms/step\n",
            "Epoch 39/40\n",
            "402/402 - 2s - loss: 0.5427 - accuracy: 0.7372 - 2s/epoch - 5ms/step\n",
            "Epoch 40/40\n",
            "402/402 - 2s - loss: 0.5435 - accuracy: 0.7361 - 2s/epoch - 4ms/step\n",
            "268/268 - 1s - loss: 0.5511 - accuracy: 0.7266 - 555ms/epoch - 2ms/step\n",
            "Loss: 0.5511290431022644, Accuracy: 0.7266472578048706\n"
          ]
        }
      ]
    }
  ]
}